> [!quote] Information 
> * @ Conference ICLR 2024 
> * paper Paper [Link](https://arxiv.org/pdf/2301.02240)
> * git Github [Link](https://github.com/NVlabs/FasterViT)
> * hf Huggingface [Link](https://huggingface.co/papers/2306.06189)
> * calendar Date 17 January 2023
> * ? Motivation: 
> 		Vision transformers models still face high computational cost due to quadratic complexity in self-attentions. In addition, it is well known that CNNs provides great local features representations, feature that Vision Transformers lack of,  since they provide global feature learning.
> *  Dataset Datasets:
> 	[[ImageNet]]
> 	[[MS COCO]]
> 	[[ADE20K]]
> 	
> * Fields Related fields: 
> 	[[Vision Transformers]]
> 	[[Towards Enhanced Efficiency]]
> 	[[Self-Attention mechanisms]]
> 	
