> [!quote] Information 
> * @ Conference NONE 
> * paper Paper [Link](https://arxiv.org/pdf/2412.13663)
> * git Github [Link](https://github.com/AnswerDotAI/ModernBERT)
> * hf Huggingface [Link](https://huggingface.co/papers/2412.13663)
> * calendar Date 19 December 2024
> * ? Motivation: 
> 		Nowadays the so called GenAI models, such as LLama, ChatGPT, etc. Are models with an insane quantity of parameters. The purpose of the paper is improve the original BERT, adding latest optimizations, thus making it possible to obtain a better version, more scalable, long context and local-global attention.    
> *  Dataset Datasets:
> 	[[ImageNet]]
> 	[[MS COCO]]
> 	[[ADE20K]]
> 	
> * Fields Related fields: 
> 	[[Vision Transformers]]
> 	[[Towards Enhanced Efficiency]]
> 	[[Self-Attention mechanisms]]

## 1. Introduction

We all rememember the BERT dominance in a big majority of NLP tasks before the appearance of LLMs. During these years of LLMs (encoder-decoder) Improvements, people     
## 2. Architecture
## 3. Data
## 4. Results
