> [!quote] Information 
> * @ Conference NONE 
> * paper Paper [Link](https://arxiv.org/pdf/2412.13663)
> * git Github [Link](https://github.com/AnswerDotAI/ModernBERT)
> * hf Huggingface [Link](https://huggingface.co/papers/2412.13663)
> * calendar Date 19 December 2024
> * ? Motivation: 
> 		Nowadays the so called GenAI models, such as LLama, ChatGPT, etc. Are models with an insane quantity of parameters. The purpose of the paper is improve the original BERT, adding latest optimizations, thus making it possible to obtain a better version, more scalable, long context and local-global attention.    
> *  Dataset Datasets:
> 	[[ImageNet]]
> 	[[MS COCO]]
> 	[[ADE20K]]
> 	
> * Fields Related fields: 
> 	[[Vision Transformers]]
> 	[[Towards Enhanced Efficiency]]
> 	[[Self-Attention mechanisms]]

